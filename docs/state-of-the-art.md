# Аналіз предметної області

## Вступ

Даний документ містить основні підходи вирішення завдання на усіх етапах життєвого циклу даних. Розглядаються підходи до управління даними та порівняння існуючих систем.

 Аналіз предметної області містить наступні розділи:
- [Основні визначення](#Основні-визначення)
- [Підходи та способи вирішення завдання](#Підходи-та-способи-вирішення-завдання)
- [Порівняльна характеристика існуючих засобів вирішення завдання](#Порівняльна-характеристика-існуючих-засобів-вирішення-завдання)
- [Висновки](#Висновки)
- [Посилання](#Посилання)

## Основні визначення

*[Розділ містить визначення термінів та скорочень, які використовуються при аналізі предметної області.]*

## Підходи та способи вирішення завдання

Системи управління відкритими даними створюють умови для ефективного використання даних, підвищення прозорості організаційної діяльності та сприяння інноваціям. Однак, щоб досягти цих цілей, необхідно впроваджувати різноманітні підходи та методи, що забезпечують належне управління даними на всіх етапах їхнього життєвого циклу.

**Життєвий цикл даних (data lifecycle)** — це процес, що охоплює всі етапи від створення даних до їх знищення або архівування. Цей цикл включає різні стадії, на кожній з яких дані проходять певні операції, що впливають на їх якість, доступність та безпеку. Основні етапи життєвого циклу даних:

1. Створення даних

2. Обробка даних

3. Зберігання даних

4. Аналіз даних

5. Безпека даних

6. Поширення даних

7. Архівування та знищення даних

### Створення даних
Це перший етап, який включає збір, генерування або отримання даних. Дані можуть створюватися через різні методи, такі як опитування, автоматизоване збирання з сенсорів, імпорт з інших систем або формування нових даних шляхом обробки існуючих. Важливо, щоб дані були зібрані у відповідності до визначених стандартів та вимог, щоб забезпечити їхню якість та точність.

#### 1. Опитування та анкетування
Опитування є одним із найпоширеніших способів збору даних. Це дозволяє отримувати інформацію безпосередньо від респондентів

- Google Forms: 
  - Безкоштовний інструмент для створення форм і опитувань
  - Легко інтегрується з Google Sheets для аналізу даних
  
- SurveyMonkey: 
  - Платформа для створення та управління опитуваннями
  - Пропонує аналітичні функції для оцінки результатів
  
- Qualtrics: 
  - Потужна платформа для створення опитувань
  - Використовується в наукових дослідженнях для збору та аналізу даних

#### 2. Веб-скрапінг (Web Crawling)
Ця техніка дозволяє витягувати дані з веб-сторінок для подальшого аналізу

- Beautiful Soup (Python): 
  - Бібліотека для парсингу HTML і XML документів, що дозволяє отримувати дані з веб-сторінок
  
- Scrapy: 
  - Фреймворк для збору даних з веб-сайтів
  - Ефективно організовує процес скрапінгу
  
- Octoparse: 
  - Інструмент для веб-скрапінгу без коду
  - Легко збирає дані з веб-сайтів за допомогою інтуїтивно зрозумілого інтерфейсу

#### 3. API (Application Programming Interface)
Використання API дозволяє отримувати дані з зовнішніх систем та сервісів

- Postman: 
  - Інструмент для тестування API, що дозволяє відправляти запити та отримувати дані
  
- Python Requests: 
  - Бібліотека для роботи з HTTP-запитами в Python
  - Дозволяє отримувати дані з API
  
- RapidAPI: 
  - Платформа, що надає доступ до тисяч API
  - Спрощує інтеграцію даних з різних джерел


### Обробка даних
На цьому етапі дані перевіряються, очищаються і трансформуються для подальшого аналізу. Це може включати видалення дублікатів, заповнення пропусків, нормалізацію даних та їх форматування. Якість даних є критично важливою на цьому етапі, оскільки неякісні дані можуть призвести до хибних висновків під час аналізу.

#### 1. Перевірка даних
- Валідація даних: використання правил та обмежень для перевірки коректності значень (наприклад, формат дати, діапазон чисел)
- Логічна перевірка: перевірка взаємозв’язків між різними полями (наприклад, дата народження не може бути пізніше дати реєстрації)

#### 2. Очищення даних
- Видалення дублікатів: виявлення і видалення повторюваних записів за допомогою алгоритмів (наприклад, методом обчислення унікальних значень)
- Заповнення пропусків: використання методів, таких як:
  - Середнє, медіана, мода для числових даних
  - Заповнення найближчим значенням для часових рядів
  - Інтерполяція для більш складних випадків

#### 3. Трансформація даних
- Нормалізація даних: зведення значень до одного масштабу без спотворення різниці в діапазонах (наприклад, перетворення значень у відсотки)
- Кодування категоріальних змінних: використання технік, таких як "one-hot encoding", для перетворення категоріальних змінних у числовий формат, щоб їх можна було використовувати в алгоритмах машинного навчання
- Форматування даних: зміна форматів даних (наприклад, дати в єдиний стандарт) для уникнення помилок під час обробки

### Зберігання даних
Після обробки дані зберігаються в базах даних або інших системах зберігання, таких як хмарні платформи. Важливо забезпечити належний рівень доступності та безпеки даних, щоб уникнути їх втрати або несанкціонованого доступу. Вибір формату зберігання (реляційна, NoSQL, файлові системи тощо) залежить від типу даних і вимог до їх використання.

#### 1. Формат зберігання
- *Реляційні бази даних*:
  - Підходять для структурованих даних, які мають чіткі зв’язки між таблицями
  - Переваги: транзакційна підтримка, потужні механізми запитів (SQL)
  - Приклади: PostgreSQL, MySQL, Oracle Database

- *NoSQL бази даних*:
  - Використовуються для неструктурованих або напівструктурованих даних
  - Переваги: гнучкість схеми, масштабованість
  - Типи NoSQL баз: 
    - Документні (MongoDB, CouchDB)
    - Ключ-значення (Redis, Amazon DynamoDB)
    - Графові (Neo4j, ArangoDB)
  
- *Файлові системи*:
  - Зберігання даних у форматі файлів на локальних або мережевих дисках
  - Підходять для великих обсягів даних, що не потребують складних запитів
  - Приклади: Amazon S3, Google Cloud Storage

#### 2. Хмарні платформи
- *Переваги хмарного зберігання*:
  - Масштабованість: можливість швидкого розширення ресурсів
  - Доступність: доступ до даних з будь-якого місця та в будь-який час
  - Резервне копіювання та відновлення: автоматизовані механізми для захисту даних
  
- *Популярні хмарні платформи*:
  - Amazon Web Services (AWS): сервіс S3 для об’єктного зберігання
  - Google Cloud Platform (GCP): Google Cloud Storage
  - Microsoft Azure: Azure Blob Storage

#### 3. Інструменти:
- *Системи управління базами даних (СУБД)*:
  - PostgreSQL: потужна реляційна СУБД з підтримкою розширень
  - MongoDB: популярна NoSQL база даних, що використовує документи
  - Cassandra: розподілена NoSQL база даних, яка забезпечує високу доступність та масштабованість

- *Хмарні сервіси*:
  - Amazon RDS: керований сервіс для реляційних баз даних
  - Firebase: платформа для зберігання даних у реальному часі

- *Інструменти резервного копіювання*:
  - Bacula: програмне забезпечення для резервного копіювання та відновлення
  - Veeam Backup: рішення для резервного копіювання віртуальних середовищ

### Аналіз даних
На цьому етапі дані аналізуються з метою отримання корисної інформації, виявлення закономірностей або підтримки прийняття рішень. Аналіз може здійснюватися різними методами, такими як статистичний аналіз, машинне навчання або візуалізація даних. Важливою складовою є інтерпретація результатів аналізу, що дозволяє зробити обґрунтовані висновки.

#### Підходи та інструменти
- *Статистичний аналіз*:
  - Використання статистичних методів для виявлення тенденцій та закономірностей.
  - Інструменти: R, Python (бібліотеки pandas, NumPy).
  
- *Машинне навчання*:
  - Застосування алгоритмів для навчання моделей на основі даних та прогнозування.
  - Інструменти: Scikit-learn, TensorFlow, Keras.

- *Візуалізація даних*:
  - Створення графіків і діаграм для наочності даних та висновків.
  - Інструменти: Tableau, Power BI, Matplotlib.

### Безпека даних
Забезпечення безпеки даних є критично важливим на всіх етапах їх життєвого циклу. Це включає фізичну безпеку (захист серверів і обладнання), а також кібербезпеку (шифрування, контроль доступу, моніторинг загроз). Важливо також мати політики та процедури для реагування на інциденти безпеки.

### Поширення даних
Після аналізу дані можуть бути надані різним користувачам або організаціям. Це може відбуватися через публікацію звітів, візуалізацій або через API для інтеграції з іншими системами. Важливо забезпечити, щоб дані були зрозумілі та доступні для споживачів, а також дотримуватися вимог щодо конфіденційності та захисту даних.

### Архівування та знищення даних
Коли дані більше не потрібні для активного використання, їх можна архівувати для зберігання на тривалий термін або безпечно знищити, якщо це необхідно. Архівування даних дозволяє зберегти їх для майбутнього використання, наприклад, для аудиту чи наукових досліджень. Процес знищення даних має відповідати законодавчим вимогам і внутрішнім політикам організації, щоб запобігти можливим витокам конфіденційної інформації.

#### Архівування даних:
- Архівування забезпечує зберігання даних на тривалий термін для потенційного подальшого використання, наприклад, для аудитів, аналізу тенденцій або наукових досліджень.

#### Безпечне знищення даних:
- Використання методів знищення, таких як деструкція дисків, перезапис даних (data wiping) або знищення за стандартами, щоб гарантувати, що дані не можуть бути відновлені.

## Порівняльна характеристика існуючих засобів вирішення завдання

*[Розділ містить опис існуючих програм, інформаційних систем, сервісів, тощо, призначених для вирішення 
завдання. Дається порівняльна характеристика властивостей FURPS:*
- *Functionality (функциональні вимоги)*
- *Usability (вимоги до зручності роботи)*
- *Reliability (вимоги до надійності)*
- *Performance (вимоги до продуктивності)*
- *Supportability (вимоги до підтримки)*

 *(у вигляді таблиці).]*

## Висновки
Аналіз сучасних платформ для управління відкритими даними, таких як CKAN, Socrata, OpenDataSoft, Dataverse та OpenGov, демонструє різноманітність підходів до публікації, каталогізації та управління даними. Кожна з цих платформ має свої переваги та недоліки, які можуть впливати на вибір конкретного рішення.

1. **Доцільність розробки нової або модифікації існуючої інформаційної системи**:
- В умовах постійного зростання обсягу даних та вимог до їх відкритості, розробка нової інформаційної системи або модернізація існуючої є необхідною для забезпечення ефективного управління даними. Системи, які не відповідають сучасним стандартам, можуть обмежувати доступність та використання даних, а також ускладнювати процеси аналізу та візуалізації.

- Вибір платформи, що найкраще відповідає потребам організації, може істотно підвищити продуктивність, доступність даних і покращити взаємодію з користувачами.


2. **Необхідність інтеграції з системами третіх сторін**:
- Інтеграція з існуючими системами та сервісами, такими як CRM, аналітичні платформи та фінансові системи, дозволяє розширити функціональність нової або модернізованої системи, поліпшити обмін даними та забезпечити більш гнучке управління інформацією.

- API, які пропонують більшість платформ, можуть значно спростити процес інтеграції, дозволяючи з'єднувати різні системи без потреби у складних налаштуваннях.

Таким чином, в умовах динамічного розвитку технологій і зростання вимог до відкритих даних, розробка нових інформаційних систем або модифікація існуючих є важливими для забезпечення високої ефективності, гнучкості та безпеки даних. Проведення аналізу предметної області необхідне для знаходження правильних підходів побудови системи управління відкритими даними та визначення необхідних функцій, які треба реалізувати у даній системі.

## Посилання

*[Розділ містить повний список всіх документів, про які згадується.]*
